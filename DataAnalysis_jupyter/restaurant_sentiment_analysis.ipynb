{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0033f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n감성 분류: 문서(텍스트 데이터)를 긍정의견 또는 부정의견으로 나누어 분류하는것\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Date: 2022.07.29\n",
    "Title: 데이터 분류\n",
    "By: Kang Jin Seong\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "'''\n",
    "감성 분류: 문서(텍스트 데이터)를 긍정의견 또는 부정의견으로 나누어 분류하는것\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bb7266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://place.map.kakao.com/85570955\n",
      "https://place.map.kakao.com/26431943\n",
      "https://place.map.kakao.com/27270313\n",
      "https://place.map.kakao.com/1238400864\n",
      "https://place.map.kakao.com/25891059\n",
      "https://place.map.kakao.com/1752978029\n",
      "https://place.map.kakao.com/1863534623\n",
      "https://place.map.kakao.com/109169564\n",
      "https://place.map.kakao.com/95713992\n",
      "https://place.map.kakao.com/25036974\n",
      "https://place.map.kakao.com/13575898\n",
      "https://place.map.kakao.com/2011092566\n",
      "https://place.map.kakao.com/27564595\n",
      "https://place.map.kakao.com/1465968863\n",
      "https://place.map.kakao.com/1503746075\n",
      "https://place.map.kakao.com/741391811\n",
      "https://place.map.kakao.com/495658881\n",
      "https://place.map.kakao.com/27504403\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 윈도우용 크롬 웹드라이버 실행 경로\n",
    "excutable_path = 'C:/Users/USER/DataAnalysis_jupyter/chromedriver.exe'\n",
    "\n",
    "#크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = 'https://map.kakao.com/'\n",
    "\n",
    "#크롬 드라이버를 사용합니다.\n",
    "driver = webdriver.Chrome(executable_path = excutable_path)\n",
    "\n",
    "#카카오 지도에 접속합니다.\n",
    "driver.get(source_url)\n",
    "\n",
    "# 검색창에 검색어를 입력합니다.\n",
    "searchbox = driver.find_element('xpath',\"//input[@id= 'search.keyword.query']\")\n",
    "searchbox.send_keys('강남역 맛집')\n",
    "\n",
    "# 검색버튼을 눌러서 결과를 가져옵니다.\n",
    "searchbutton = driver.find_element('xpath', \"//button[@id = 'search.keyword.submit']\")\n",
    "driver.execute_script('arguments[0].click();', searchbutton)\n",
    "\n",
    "# 검색 결과를 가져올 시간을 기다립니다.\n",
    "time.sleep(2)\n",
    "\n",
    "# 검색 결과의 페이지 소스를 가져옵니다.\n",
    "html = driver.page_source\n",
    "\n",
    "# BeautifulSoup을 이용하여 html 정보를 파싱합니다.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "moreviews = soup.find_all(name = 'a', attrs = {'class':'moreview'})\n",
    "\n",
    "# a 태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\n",
    "page_urls = []\n",
    "\n",
    "for moreview in moreviews:\n",
    "    page_url = moreview.get('href')\n",
    "    print(page_url)\n",
    "    page_urls.append(page_url)\n",
    "    \n",
    "# 크롤링에 사용한 브라우저를 종료합니다.\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fdbf05b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     contents_div \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation_review\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     print(contents_div)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 별점을 가져옵니다.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#     rates = contents_div.find_all(name = 'em', attrs = {\"class\": \"num_rate\"})\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     rates \u001b[38;5;241m=\u001b[39m \u001b[43mcontents_div\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtxt_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# 리뷰를 가져옵니다.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     reviews \u001b[38;5;241m=\u001b[39m contents_div\u001b[38;5;241m.\u001b[39mfind_all(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtxt_comment\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "가져온 맛집 리스트의 리뷰 정보 크롤링하기\n",
    "'''\n",
    "\n",
    "columns = ['score', 'review']\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = excutable_path)\n",
    "\n",
    "for page_url in page_urls:\n",
    "    \n",
    "    # 상세 보기 페이지에 접속합니다.\n",
    "    driver.get(page_url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 첫 페이지 리뷰를 크롤링합니다.\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    contents_div = soup.find(name = 'div', attrs = {'class':'evaluation_review'})\n",
    "#     print(contents_div)\n",
    "    # 별점을 가져옵니다.\n",
    "\n",
    "#     rates = contents_div.find_all(name = 'em', attrs = {\"class\": \"num_rate\"})\n",
    "    rates = contents_div.find_all(name = 'span', attrs = {\"class\": \"txt_desc\"})\n",
    "    # 리뷰를 가져옵니다.\n",
    "    reviews = contents_div.find_all(name = 'p', attrs = {\"class\": \"txt_comment\"})\n",
    "    \n",
    "    for rate, review in zip(rates, reviews):\n",
    "        row = [rate.text[0], review.find(name = 'span').text]\n",
    "        series = pd.Series(row, index = df.columns)\n",
    "        df = df.append(series, ignore_index = True)\n",
    "#         print(df)\n",
    "        \n",
    "    # 2-5페이지의 리뷰를 크롤링합니다.\n",
    "    for button_num in range(2,6):\n",
    "        # 오류가 나는 경우 (리뷰 페이지가 없는 경우) 수행하지 않습니다.\n",
    "        \n",
    "        try:\n",
    "            another_reviews = driver.find_element('xpath', \"//a[@data-page  = '\" + str(button_num) + \"']\")\n",
    "            another_reviews.click()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 페이지 리뷰를 크롱링합니다.\n",
    "            \n",
    "            html = driverr.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            contents_div = soup.find(name = 'div', attrs = {\"class\": \"evaluation_review\"})\n",
    "            \n",
    "            # 별점을 가져옵니다.\n",
    "#             rates = contents_div.find_all(name = 'em', attrs = {\"class\": \"num_rate\"})\n",
    "            rates = contents_div.find_all(name = 'span', attrs = {\"class\": \"txt_desc\"})\n",
    "            \n",
    "            # 리뷰를 가져옵니다.\n",
    "            reviews = contents_div.find_all(name = 'p', attrs = {\"class\": \"txt_comment\"})\n",
    "            \n",
    "            for rate, review in zip(rates, reviews):\n",
    "                row = [rate.text[0], review.find(name = 'span').text]\n",
    "                series = pd.Series(row, index = df.columns)\n",
    "                df = df.append(series, ignore_index = True)\n",
    "#                 print(df)\n",
    "                \n",
    "        except:\n",
    "#             print('end')\n",
    "            break\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a740f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4점 이상의 리뷰는 긍정, 3점 이하의 리뷰는 부정으로 평가합니다.\n",
    "df['y'] = df['score'].apply(lambda x: 1 if float(x) > 3 else 0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"review_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TF-IDF를 이용한 핵심어 추출\n",
    " - 형태소 추출하기(한글 텍스트로 전처리)\n",
    "'''\n",
    "df = pd.read_csv(\"review_data.csv\")\n",
    "\n",
    "import re\n",
    "\n",
    "# 텍스트 정제 함수 : 한글 이외의 문자는 전부 제거\n",
    "def text_cleaning(text):\n",
    "    # 한글의 정규표현식으로 한글만 추출합니다.\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    result = hangul.sub('', str(text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d982332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 적용하여 리뷰에서 한글만 추출합니다.\n",
    "df = pd.read_csv(\"review_data.csv\")\n",
    "df['ko_text'] = df['review'].apply(lambda x: text_cleaning(x))\n",
    "del df['review']\n",
    "\n",
    "# 한 글자 이상의 텍스트를 가지고 있는 데이터만 추출합니다.\n",
    "df = df[df['ko_text'].str.len() > 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''형태소 단위로 추출'''\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# konlpy 라이브러리로 텍스트 데이터에서 형태소를 추출합니다.\n",
    "def get_pos(x):\n",
    "    tagger = Okt()\n",
    "    pos = tagger.pos(x)\n",
    "    pos = ['{}/{}'.format(word, tag) for word, tag in pos]\n",
    "    return pos\n",
    "\n",
    "# 형태소 추출 동작을 테스트 합니다.\n",
    "result = get_pos(df['ko_text'].values[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Date: 2022.08.08\n",
    "\n",
    "- 분류 모델의 학습 데이터로 변환하기\n",
    "  - raw 데이터셋: 데이터 프레임의 텍스트 데이터에 해당합니다.\n",
    "  - 말뭉치: raw 데이터셋으로부터 말뭉치를 생성합니다. 이 말뭉치는 형태소의 서로 다른 고유 한 셋을 가지고 있습니다.\n",
    "  - 학습 데이터 셋: 서로다른 형태소는 각 텍스트 데이터의 벡터 길이가 됩니다.\n",
    "                    만약 텍스트에 해당 단어가 존재하면 벡터의 값을 1로, 존재하지 않으면 벡터의 값을 0으로 할당합니다.(p242 참조)\n",
    "\n",
    "'''\n",
    "# corpus index 생성\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 형태소를 벡터 형태의 학습 데이터 셋(X데이터)으로 변환합니다.\n",
    "index_vectorizer = CountVectorizer(tokenizer = lambda x: get_pos(x))\n",
    "X = index_vectorizer.fit_transform(df['ko_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76926cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(index_vectorizer.vocabulary_)[:100]+\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['ko_text'].values[0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79816050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF(Term Frequency - Inverse Documnet Frequency)\n",
    "# 단어의 빈도를 나타내는 TF 와 문서 빈도를 나타내는 DF의 역수인 IDF를 곱한 값을 의미힌다.\n",
    "# 이는 다른 문서들에서는 등장하지 않았지만 현재 문서에서는 많이 등장하는 단어를 의미하며 그 단어가 현재 문서에서 얼마나 중요한지를\n",
    "# 피처로 나타낼수 있는 방법입니다.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# TF-IDF 방법으로 형태소를 벡터 형태의 학습 데이터셋(X 데이터)으로 변환합니다.\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "X = tfidf_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596630a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''긍정 부정 리뷰 분류하기'''\n",
    "\n",
    "# 분류 모델링(데이터셋 분리)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['y']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.30)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 로지스틱 회귀 모델을 학습합니다.\n",
    "lr = LogisticRegression(random_state = 0)\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "y_pred_probability = lr.predict_proba(x_test)[:,1]\n",
    "\n",
    "# 로지스틱 회귀모델의 성능을 평가합니다.\n",
    "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"Precision: %.3f\" % precision_score(y_test,y_pred))\n",
    "print(\"Recall: %.3f\" % recall_score(y_test, y_pred))\n",
    "print(\"F1: %.3f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix를 출력합니다.\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 불균형 문제 해결하기\n",
    "\n",
    "# y가 0과 1을 각각 얼마나 가지고 있는지를 출력합니다.\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# AUC를 계산합니다.\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_probability)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probability)\n",
    "print(\"AUC: %.3f\" % roc_auc)\n",
    "\n",
    "# ROC Curve 그래프를 출력합니다.\n",
    "plt.rcParams['figure.figsize'] = [5,4]\n",
    "plt.plot(false_positive_rate, true_positive_rate, label = \"ROC curve (area = %0.3f)\" % roc_auc, color= 'red', linewidth = 4.0)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve of Logistic regression')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''중요 키워드 분석하기'''\n",
    "\n",
    "# 회귀 모델의 피처 영향력 추출\n",
    "# 학습한 회귀 모델의 계수를 출력합니다.\n",
    "plt.rcParams['figure.figsize'] = [10,8]\n",
    "plt.bar(range(len(lr.coef_[0])), lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse = True)[:5])\n",
    "print(sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse = True)[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fada353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요 피처의 형태소\n",
    "# 회귀 모델의 계수를 높은 순으로 정렬합니다.\n",
    "coef_pos_index = sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 모델의 계수를 index_vectorize 에 맵핑하여, 어떤 형태소인지 출력 할 수 있게 합니다.\n",
    "invert_index_vectorizer = {v: k for k, v in index_vectorizer.vocabulary_.items()}\n",
    "\n",
    "# 계수가 높은 순으로, 피처에 형태소를 맵핑한 결과를 출력합니다. 계수가 높은 피처는 리뷰에 긍정적인 영향을 주는 형태소라고 할 수 있습니다.\n",
    "print(str(invert_index_vectorizer)[:100] + '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d5c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 20개 긍정 형태소를 출력합니다.\n",
    "for coef in coef_pos_index[:20]:\n",
    "    print(invert_index_vectorizer[coef[1]], coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b9779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
